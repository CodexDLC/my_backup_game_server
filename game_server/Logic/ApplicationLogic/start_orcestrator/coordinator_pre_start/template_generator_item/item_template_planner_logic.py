# game_server/Logic/ApplicationLogic/start_orcestrator/coordinator_pre_start/template_generator_item/item_template_planner_logic.py

# -*- coding: utf-8 -*-
import json
import asyncio
import re
import time
import hashlib
import uuid
from typing import Dict, Any, Optional, List, Set, Tuple, Callable

# –ì–ª–∞–≤–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è –≤—Å–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
from game_server.Logic.CoreServices.services.data_version_manager import DataVersionManager
from game_server.config.provider import config

# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–æ–Ω—Ñ–∏–≥–∞–º–∏
from game_server.Logic.ApplicationLogic.start_orcestrator.start_orcestrator_utils.generator_utils import generate_item_code

# –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç RepositoryManager
from game_server.Logic.InfrastructureLogic.app_post.repository_manager import RepositoryManager


# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∫—ç—à-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ (—Ç–∏–ø–∏–∑–∞—Ü–∏—è)
from game_server.Logic.InfrastructureLogic.app_cache.central_redis_client import CentralRedisClient # –î–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏
from game_server.Logic.InfrastructureLogic.app_cache.services.task_queue.redis_batch_store import RedisBatchStore
from game_server.Logic.InfrastructureLogic.app_cache.services.task_queue.task_queue_cache_manager import ITaskQueueCacheManager # –î–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏
from game_server.Logic.InfrastructureLogic.app_cache.services.reference_data.reference_data_reader import IReferenceDataReader # –î–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏
from arq.connections import ArqRedis # –î–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏ arq_redis_pool

# –ù–ï –ú–ï–ù–Ø–¢–¨: from game_server.database.models.models import EquipmentTemplate # –û—Å—Ç–∞–≤–∏—Ç—å, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–º
from game_server.Logic.InfrastructureLogic.logging.logging_setup import app_logger as logger

from game_server.Logic.ApplicationLogic.start_orcestrator.start_orcestrator_utils.batch_utils import split_into_batches

# –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç ItemGenerationSpec DTO
from game_server.common_contracts.start_orcestrator.dtos import ItemGenerationSpec #


class ItemTemplatePlannerLogic:
    def __init__(
        self,
        repository_manager: RepositoryManager, # –ò–ó–ú–ï–ù–ï–ù–û: —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ–º RepositoryManager
        central_redis_client: CentralRedisClient, # –ò–ó–ú–ï–ù–ï–ù–û: —Ç–æ—á–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
        reference_data_reader: IReferenceDataReader, # –ò–ó–ú–ï–ù–ï–ù–û: —Ç–æ—á–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
        task_queue_cache_manager: ITaskQueueCacheManager, # –ò–ó–ú–ï–ù–ï–ù–û: —Ç–æ—á–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
        arq_redis_pool: ArqRedis, # –ò–ó–ú–ï–ù–ï–ù–û: —Ç–æ—á–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
        item_generation_limit: Optional[int] = None
    ):
        # –£–î–ê–õ–ï–ù–û: self.async_session_factory = async_session_factory
        self.repository_manager = repository_manager # –î–û–ë–ê–í–õ–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º RepositoryManager
        self.logger = logger
        self.central_redis_client = central_redis_client
        self.reference_data_reader = reference_data_reader
        self.task_queue_cache_manager = task_queue_cache_manager
        self.arq_redis_pool = arq_redis_pool # –≠—Ç–æ—Ç –∞—Ç—Ä–∏–±—É—Ç –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è enqueue_job –∑–¥–µ—Å—å.
        
        self.redis_batch_store = RedisBatchStore(redis_client=self.central_redis_client) # RedisBatchStore –ø—Ä–∏–Ω–∏–º–∞–µ—Ç CentralRedisClient

        self.item_generation_limit = item_generation_limit
        logger.debug("‚úÖ ItemTemplatePlannerLogic –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")

    async def _get_current_data_fingerprint(self) -> str:
        dependency_keys = [
            config.constants.redis.REDIS_KEY_GENERATOR_ITEM_BASE,
            config.constants.redis.REDIS_KEY_GENERATOR_MATERIALS,
            config.constants.redis.REDIS_KEY_GENERATOR_SUFFIXES,
        ]
        versions = await DataVersionManager.get_redis_fingerprint(self.central_redis_client, dependency_keys)
        fingerprint_str = json.dumps(versions, sort_keys=True)
        return hashlib.sha256(fingerprint_str.encode('utf-8')).hexdigest()

    async def _get_cached_etalon_pool(self) -> Tuple[Optional[Dict], Optional[str]]:
        try:
            async with self.central_redis_client.pipeline() as pipe:
                pipe.get(config.constants.redis.REDIS_KEY_ETALON_ITEM_POOL)
                pipe.get(config.constants.redis.REDIS_KEY_ETALON_ITEM_FINGERPRINT)
                results = await pipe.execute()
            
            pool_json, fingerprint_hash = results[0], results[1]
            if not pool_json or not fingerprint_hash:
                return None, None
            
            # –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ –∫—ç—à–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ ItemGenerationSpec –æ–±—ä–µ–∫—Ç—ã
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤ –∫—ç—à–µ –æ–Ω–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–∞–∫ —Å–ª–æ–≤–∞—Ä–∏ (json.loads)
            cached_pool_dicts = json.loads(pool_json)
            cached_pool_specs = {k: ItemGenerationSpec(**v) for k, v in cached_pool_dicts.items()} # <--- –î–û–ë–ê–í–õ–ï–ù–û
            
            return {k: spec.model_dump() for k, spec in cached_pool_specs.items()}, fingerprint_hash # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å —Å–∏–≥–Ω–∞—Ç—É—Ä—É –º–µ—Ç–æ–¥–∞ get_or_build_etalon_specs –ø–æ–∫–∞ —á—Ç–æ.
                                                                                                # –í –∏–¥–µ–∞–ª–µ, etalon_specs –¥–æ–ª–∂–Ω—ã —Å—Ç–∞—Ç—å Dict[str, ItemGenerationSpec]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞: {e}", exc_info=True)
            return None, None

    async def _cache_etalon_pool(self, pool: Dict, fingerprint_hash: str):
        try:
            # –ò–ó–ú–ï–ù–ï–ù–û: –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ pool —Å–æ–¥–µ—Ä–∂–∏—Ç Pydantic –æ–±—ä–µ–∫—Ç—ã, –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            # pool_to_cache = {k: v.model_dump() if isinstance(v, ItemGenerationSpec) else v for k, v in pool.items()} # –ï—Å–ª–∏ pool —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ItemGenerationSpec
            pool_to_cache = pool # –í —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ pool —É–∂–µ Dict[str, Tuple], –ø–æ—ç—Ç–æ–º—É model_dump –Ω–µ –Ω—É–∂–µ–Ω
                                 # –ù–æ –µ—Å–ª–∏ –±—ã etalon_item_data_specs —Ö—Ä–∞–Ω–∏–ª ItemGenerationSpec, —Ç–æ–≥–¥–∞ –±—ã pool_to_cache
                                 # –±—ã–ª –±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ dict.
            pool_json = json.dumps(pool_to_cache, ensure_ascii=False) # –ò—Å–ø–æ–ª—å–∑—É–µ–º pool_to_cache
            
            async with self.central_redis_client.pipeline() as pipe:
                pipe.set(config.constants.redis.REDIS_KEY_ETALON_ITEM_POOL, pool_json)
                pipe.set(config.constants.redis.REDIS_KEY_ETALON_ITEM_FINGERPRINT, fingerprint_hash)
                pipe.expire(config.constants.redis.REDIS_KEY_ETALON_ITEM_POOL, config.settings.prestart.ETALON_POOL_TTL_SECONDS)
                pipe.expire(config.constants.redis.REDIS_KEY_ETALON_ITEM_FINGERPRINT, config.settings.prestart.ETALON_POOL_TTL_SECONDS)
                await pipe.execute()
            self.logger.info(f"‚úÖ –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –ø—É–ª –ø—Ä–µ–¥–º–µ—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω (—Ö—ç—à: {fingerprint_hash[:8]}...).")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞: {e}", exc_info=True)

    # –ò–ó–ú–ï–ù–ï–ù–û: –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Dict[str, ItemGenerationSpec]
    async def get_or_build_etalon_specs(self) -> Dict[str, ItemGenerationSpec]:
        self.logger.debug("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –¥–ª—è —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞ –ø—Ä–µ–¥–º–µ—Ç–æ–≤...")
        
        current_fingerprint = await self._get_current_data_fingerprint()
        cached_pool_dicts, cached_fingerprint = await self._get_cached_etalon_pool() # cached_pool_dicts —É–∂–µ –±—É–¥—É—Ç —Å–ª–æ–≤–∞—Ä—è–º–∏

        if cached_pool_dicts is not None and current_fingerprint == cached_fingerprint:
            self.logger.debug(f"‚úÖ –ö—ç—à —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞ –∞–∫—Ç—É–∞–ª–µ–Ω (—Ö—ç—à: {current_fingerprint[:8]}...). –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é.")
            # –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ –∫—ç—à–∞ –≤ ItemGenerationSpec –æ–±—ä–µ–∫—Ç—ã
            return {k: ItemGenerationSpec(**v) for k, v in cached_pool_dicts.items()} # <--- –î–û–ë–ê–í–õ–ï–ù–û
        
        self.logger.warning(f"–ö—ç—à —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª. –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞... (–¢–µ–∫—É—â–∏–π —Ö—ç—à: {current_fingerprint[:8]}, –∫—ç—à: {cached_fingerprint[:8] if cached_fingerprint else 'N/A'})")
        
        item_base_raw, materials_raw, suffixes_raw, modifiers_raw = await self.load_reference_data_from_redis()
        
        item_base = item_base_raw if item_base_raw is not None else {}
        materials = materials_raw if materials_raw is not None else {}
        suffixes = suffixes_raw if suffixes_raw is not None else {}
        modifiers = modifiers_raw if modifiers_raw is not None else {}
        
        # –ò–ó–ú–ï–ù–ï–ù–û: build_etalon_item_codes —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Dict[str, ItemGenerationSpec]
        new_etalon_pool: Dict[str, ItemGenerationSpec] = self.build_etalon_item_codes(item_base, materials, suffixes)
        
        existing_codes_in_db = await self.get_existing_item_codes_from_db()
        new_etalon_item_codes = set(new_etalon_pool.keys())

        item_codes_to_delete = existing_codes_in_db - new_etalon_item_codes

        if item_codes_to_delete:
            deleted_count = await self._perform_item_deletions(list(item_codes_to_delete))
            self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} '–ª–∏—à–Ω–∏—Ö' item_code –∏–∑ –ë–î.")
        else:
            self.logger.debug("‚ÑπÔ∏è –ù–µ—Ç '–ª–∏—à–Ω–∏—Ö' item_code –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –ë–î.")

        # –ò–ó–ú–ï–ù–ï–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        await self._cache_etalon_pool({k: v.model_dump() for k, v in new_etalon_pool.items()}, current_fingerprint) # <--- –î–û–ë–ê–í–õ–ï–ù–û
        
        return new_etalon_pool

    async def load_reference_data_from_redis(self) -> tuple[Dict, Dict, Dict, Dict]:
        self.logger.debug("ItemTemplatePlannerLogic: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Redis —á–µ—Ä–µ–∑ ReferenceDataReader...")
        item_base_result, materials_result, suffixes_result, modifiers_result = await asyncio.gather(
            self.reference_data_reader.get_all_item_bases(),
            self.reference_data_reader.get_all_materials(),
            self.reference_data_reader.get_all_suffixes(),
            self.reference_data_reader.get_all_modifiers()
        )
        
        item_base = item_base_result if item_base_result is not None else {}
        materials = materials_result if materials_result is not None else {}
        suffixes = suffixes_result if suffixes_result is not None else {}
        modifiers = modifiers_result if modifiers_result is not None else {}
        
        self.logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(item_base)} item_base, {len(materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, {len(suffixes)} —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤, {len(modifiers)} –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤.")
        return item_base, materials, suffixes, modifiers

    async def get_existing_item_codes_from_db(self) -> Set[str]:
        repo = self.repository_manager.equipment_templates
        existing_codes = await repo.get_all_item_codes()
        self.logger.debug(f"ItemTemplatePlannerLogic: –ü–æ–ª—É—á–µ–Ω–æ {len(existing_codes)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö item_code –∏–∑ –ë–î.")
        return set(existing_codes)

    async def _perform_item_deletions(self, item_codes_to_delete: List[str]) -> int:
        if not item_codes_to_delete:
            return 0
        
        try:
            repo = self.repository_manager.equipment_templates
            # –ï—Å–ª–∏ delete_by_item_code_batch –µ—â–µ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –Ω—É–∂–Ω–æ –µ–≥–æ –¥–æ–±–∞–≤–∏—Ç—å
            deleted_count = await repo.delete_by_item_code_batch(item_codes_to_delete)
            self.logger.info(f"üóëÔ∏è –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {deleted_count} '–ª–∏—à–Ω–∏—Ö' item_code –∏–∑ –ë–î.")
            return deleted_count
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ item_code –∏–∑ –ë–î: {e}", exc_info=True)
            return 0

    # –ò–ó–ú–ï–ù–ï–ù–û: –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Dict[str, ItemGenerationSpec]
    def build_etalon_item_codes(
        self, item_base_data: Dict[str, Any], materials_data: Dict[str, Any], suffixes_data: Dict[str, Any]
    ) -> Dict[str, ItemGenerationSpec]: # <--- –ò–ó–ú–ï–ù–ï–ù–û
        etalon_item_data_specs: Dict[str, ItemGenerationSpec] = {} # <--- –ò–ó–ú–ï–ù–ï–ù–û
        self.logger.info("ItemTemplatePlannerLogic: –ù–∞—á–∞–ª–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∞ item_code.")
        start_time = time.time()

        if not item_base_data or not materials_data or not suffixes_data:
            self.logger.warning("–û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–±–æ—Ä–æ–≤ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç—ã. –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –ø—É–ª –Ω–µ –±—É–¥–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
            return etalon_item_data_specs

        for base_code, base_info in item_base_data.items():
            category = base_info.get('category', 'UNKNOWN_CATEGORY')
            specific_names_map = base_info.get('names', {})
            if not specific_names_map: continue

            for original_specific_name, name_properties in specific_names_map.items():
                allowed_suffix_groups = set(name_properties.get('allowed_suffix_groups', []))

                for material_code, material_info in materials_data.items():
                    material_rarity_level = material_info.get('rarity_level')
                    rarity_level_to_use = config.constants.item.DEFAULT_RARITY_LEVEL if material_rarity_level is None else int(material_rarity_level)

                    material_type = material_info.get('type')
                    if not material_type: continue

                    category_rules = config.constants.item.MATERIAL_COMPATIBILITY_RULES.get(category, config.constants.item.MATERIAL_COMPATIBILITY_RULES["UNKNOWN_CATEGORY"])
                    if material_type in category_rules.get("disallowed_types", []) or material_type not in category_rules.get("allowed_types", []):
                        continue

                    for suffix_code, suffix_info in suffixes_data.items():
                        is_suffix_allowed = (suffix_code == "BASIC_EMPTY" or
                                             (suffix_info and suffix_info.get('group') and suffix_info.get('group') in allowed_suffix_groups))
                        if not is_suffix_allowed: continue

                        specific_name_for_code = re.sub(r'[^A-Z0-9]+', '-', original_specific_name).strip('-').upper()

                        item_code = generate_item_code(
                            category=category, base_code=base_code, specific_name=specific_name_for_code,
                            material_code=material_code, suffix_code=suffix_code, rarity_level=rarity_level_to_use
                        )

                        # –ò–ó–ú–ï–ù–ï–ù–û: –°–æ–∑–¥–∞–µ–º ItemGenerationSpec –æ–±—ä–µ–∫—Ç
                        spec_obj = ItemGenerationSpec(
                            item_code=item_code,
                            category=category,
                            base_code=base_code,
                            specific_name_key=original_specific_name,
                            material_code=material_code,
                            suffix_code=suffix_code,
                            rarity_level=rarity_level_to_use
                        )
                        etalon_item_data_specs[item_code] = spec_obj # <--- –¢–µ–ø–µ—Ä—å —Ö—Ä–∞–Ω–∏–º –æ–±—ä–µ–∫—Ç DTO

        elapsed_time = time.time() - start_time
        self.logger.info(f"ItemTemplatePlannerLogic: –ü–æ—Å—Ç—Ä–æ–µ–Ω —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –ø—É–ª –∏–∑ {len(etalon_item_data_specs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö item_code. –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥.")
        return etalon_item_data_specs

    # –ò–ó–ú–ï–ù–ï–ù–û: –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç List[ItemGenerationSpec]
    def find_missing_specs(self, etalon_specs: Dict[str, ItemGenerationSpec], existing_codes: Set[str]) -> List[ItemGenerationSpec]: # <--- –ò–ó–ú–ï–ù–ï–ù–û
        return [spec for item_code, spec in etalon_specs.items() if item_code not in existing_codes]

    # –ò–ó–ú–ï–ù–ï–ù–û: –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç List[ItemGenerationSpec]
    async def prepare_tasks_for_missing_items(
        self, missing_specs: List[ItemGenerationSpec], item_generation_limit: Optional[int] # <--- –ò–ó–ú–ï–ù–ï–ù–û
    ) -> List[Dict[str, Any]]:
        self.logger.debug(f"–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–¥–∞—á –¥–ª—è {len(missing_specs)} –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤.")

        specs_to_process = missing_specs
        if item_generation_limit is not None and item_generation_limit >= 0:
            specs_to_process = missing_specs[:item_generation_limit]
            self.logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω –ª–∏–º–∏—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(specs_to_process)} —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π.")

        if not specs_to_process:
            self.logger.info("–ù–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á.")
            return []

        generated_task_entries = []

        # split_into_batches —Ç–µ–ø–µ—Ä—å –±—É–¥–µ—Ç –ø–æ–ª—É—á–∞—Ç—å List[ItemGenerationSpec]
        for batch_specs_objs in split_into_batches(specs_to_process, config.settings.prestart.ITEM_GENERATION_BATCH_SIZE): # <--- –ò–ó–ú–ï–ù–ï–ù–û: –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏

            # –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ ItemGenerationSpec –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è Redis/JSON
            batch_specs_as_dicts = [
                spec_obj.model_dump(by_alias=True) # –ò–°–ü–û–õ–¨–ó–£–ï–ú Pydantic –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ dict
                for spec_obj in batch_specs_objs # <--- –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–∫—Ç—ã ItemGenerationSpec
            ]

            redis_worker_batch_id = str(uuid.uuid4())
            
            self.logger.critical(f"*** DEBUG_ENQUEUE_ITEM_PRE: Batch ID='{redis_worker_batch_id}', Chunk Size={len(batch_specs_as_dicts)}")

            success = await self.task_queue_cache_manager.add_task_to_queue(
                batch_id=redis_worker_batch_id,
                key_template=config.constants.redis.ITEM_GENERATION_REDIS_TASK_KEY_TEMPLATE,
                specs=batch_specs_as_dicts, # –ó–¥–µ—Å—å –≤—Å–µ –µ—â–µ –æ–∂–∏–¥–∞—é—Ç—Å—è —Å–ª–æ–≤–∞—Ä–∏, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è Redis/ARQ
                target_count=len(batch_specs_as_dicts),
                initial_status="pending"
            )

            if success:
                generated_task_entries.append({"batch_id": redis_worker_batch_id})
                self.logger.info(f"–ë–∞—Ç—á ID '{redis_worker_batch_id}' —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ Redis. –û–∂–∏–¥–∞–µ—Ç –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ ARQ.")
            else:
                self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞—Ç—á ID '{redis_worker_batch_id}' –≤ Redis. ARQ-–∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞.")

        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(generated_task_entries)} –±–∞—Ç—á–µ–π –∑–∞–¥–∞—á –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–µ–¥–º–µ—Ç–æ–≤. –ì–æ—Ç–æ–≤ –∫ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –≤ –æ—á–µ—Ä–µ–¥—å ARQ.")
        return generated_task_entries