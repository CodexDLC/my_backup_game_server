# game_server/Logic/InfrastructureLogic/messaging/rabbit_utils.py

import uuid
import json
import logging
from typing import List, Dict, Any, Callable, Optional, Iterator

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è Redis –∏ RabbitMQ
from game_server.Logic.InfrastructureLogic.app_cache.central_redis_client import central_redis_client


logger = logging.getLogger(__name__)

def split_into_batches(data: List[Any], batch_size: int) -> Iterator[List[Any]]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ –±–∞—Ç—á–∏ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    if not data or batch_size <= 0: return
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

# üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω task_key_template –≤ —Å–∏–≥–Ω–∞—Ç—É—Ä—É —Ñ—É–Ω–∫—Ü–∏–∏ üî•
# üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–Ω—ã –ª–∏—à–Ω–∏–µ –ª–æ–≥–∏ –æ—Ç–ª–∞–¥–∫–∏ üî•
async def save_batch_data_to_redis(
    batch_id: str,
    batch_specs: List[Dict[str, Any]],
    redis_task_key_template: str, # <--- üî• –î–û–ë–ê–í–õ–ï–ù–û üî•
    ttl_seconds: int 
) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞ –≤ Redis.
    """
    redis_task_key = redis_task_key_template.format(batch_id=batch_id)
    try:
        specs_json = json.dumps(batch_specs)
        
        await central_redis_client.hmset(redis_task_key, {
            "status": "initiation",
            "specs_json": specs_json,
            "target_count_in_chunk": len(batch_specs),
            "generated_count_in_chunk": 0
        })
        await central_redis_client.expire(redis_task_key, ttl_seconds)
        
        # logger.debug(f"–ë–∞—Ç—á '{batch_id}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ Redis: {redis_task_key} —Å TTL {ttl_seconds}s.") # –£–î–ê–õ–ï–ù–û/–ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –±–∞—Ç—á–∞ '{batch_id}' –≤ Redis: {e}", exc_info=True)
        return False


class TaskDispatcher:
    # ... (–º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞–ª–∏—Å—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–Ω—ã –ª–∏—à–Ω–∏–µ –ª–æ–≥–∏ –æ—Ç–ª–∞–¥–∫–∏ –∏–∑ process_and_dispatch_tasks üî•
    async def process_and_dispatch_tasks(
        self,
        task_list: List[Dict[str, Any]],
        batch_size: int,
        redis_task_key_template: str,
        redis_ttl_seconds: int,
        celery_queue_name: str,
        celery_task_callable: Callable[[str], Any],
        task_type_name: str
    ) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –Ω–∞ –±–∞—Ç—á–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Redis –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ Celery.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—ã—Ä—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –∑–∞–¥–∞—á.
        """
        if not task_list:
            logger.info(f"–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è {task_type_name}. –ü—Ä–æ–ø—É—Å–∫ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏.")
            return []

        logger.info(f"–ù–∞—á–∞–ª–æ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏ {len(task_list)} –∑–∞–¥–∞—á –¥–ª—è {task_type_name}...")
        
        worker_batch_chunks = list(split_into_batches(task_list, batch_size))
        logger.info(f"–ó–∞–¥–∞—á–∏ {task_type_name} —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(worker_batch_chunks)} —Ä–∞–±–æ—á–∏—Ö –±–∞—Ç—á–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ {batch_size}.")

        created_batch_ids: List[str] = []
        for i, chunk_of_specs in enumerate(worker_batch_chunks):
            if not chunk_of_specs:
                continue
            
            redis_worker_batch_id = str(uuid.uuid4())
            
            success = await save_batch_data_to_redis(
                batch_id=redis_worker_batch_id,
                batch_specs=chunk_of_specs,
                redis_task_key_template=redis_task_key_template, # –ü–µ—Ä–µ–¥–∞–µ–º task_key_template
                ttl_seconds=redis_ttl_seconds
            )
            
            if success:
                try:
                    celery_task_callable.apply_async(
                        args=[redis_worker_batch_id], 
                        queue=celery_queue_name
                    )
                    created_batch_ids.append(redis_worker_batch_id)
                    logger.info(f"–ó–∞–¥–∞—á–∞ –¥–ª—è –±–∞—Ç—á–∞ {i+1}/{len(worker_batch_chunks)} ({task_type_name}) ID '{redis_worker_batch_id}' —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å '{celery_queue_name}'.")
                except Exception as e:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –¥–ª—è –±–∞—Ç—á–∞ ID '{redis_worker_batch_id}' ({task_type_name}) –≤ –æ—á–µ—Ä–µ–¥—å Celery: {e}", exc_info=True)
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞—Ç—á ID '{redis_worker_batch_id}' ({task_type_name}) –≤ Redis. Celery-–∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞.")
        
        logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á –¥–ª—è {task_type_name}. –í—Å–µ–≥–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {len(created_batch_ids)} –±–∞—Ç—á–µ–π.")
        return created_batch_ids

    # ... (dispatch_existing_batch_id, send_raw_message –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...