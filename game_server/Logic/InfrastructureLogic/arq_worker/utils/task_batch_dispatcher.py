# game_server\Logic\InfrastructureLogic\arq_worker\utils\task_batch_dispatcher.py

import uuid # –í—Å–µ –µ—â–µ –Ω—É–∂–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ batch_id
# import json # –£–î–ê–õ–ï–ù–û: –ë–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω, –µ—Å–ª–∏ –≤—Å–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ TaskQueueCacheManager
import logging
from typing import List, Dict, Any, Callable, Optional, Iterator
from arq.connections import ArqRedis # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏ arq_redis_pool

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π CentralRedisClient
from game_server.Logic.InfrastructureLogic.app_cache.central_redis_client import CentralRedisClient
from game_server.config.constants.redis import KEY_PREFIX_TASK_QUEUE # –í—Å–µ –µ—â–µ –Ω—É–∂–µ–Ω –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª—é—á–∞

# –ò–ú–ü–û–†–¢–ò–†–£–ï–ú TaskQueueCacheManager –∏–∑ –Ω–æ–≤–æ–π –¥–æ–º–µ–Ω–Ω–æ–π –ø–∞–ø–∫–∏
from game_server.Logic.InfrastructureLogic.app_cache.services.task_queue.task_queue_cache_manager import TaskQueueCacheManager # –ò–ó–ú–ï–ù–ï–ù–û
# –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ TaskQueueCacheManager –¥–ª—è —Ç–∏–ø–∏–∑–∞—Ü–∏–∏
from game_server.Logic.InfrastructureLogic.app_cache.interfaces.interfaces_task_queue_cache import ITaskQueueCacheManager


logger = logging.getLogger(__name__)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã ---

def split_into_batches(data: List[Any], batch_size: int) -> Iterator[List[Any]]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ –±–∞—Ç—á–∏ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    if not data or batch_size <= 0:
        return iter([]) # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]


class ArqTaskDispatcher: # –ö–ª–∞—Å—Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ ArqTaskDispatcher
    """
    –ö–ª–∞—Å—Å-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á –≤ ARQ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TaskQueueCacheManager –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π.
    """
    # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã–π ArqRedis –ø—É–ª –∏ CentralRedisClient
    def __init__(self, arq_redis_pool: ArqRedis, redis_client: CentralRedisClient):
        self.arq_redis_pool = arq_redis_pool
        self.redis_client = redis_client # –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ TaskQueueCacheManager
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TaskQueueCacheManager –∑–¥–µ—Å—å
        # –ò–ó–ú–ï–ù–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–µ—Å–ª–∏ TaskQueueCacheManager —Ä–µ–∞–ª–∏–∑—É–µ—Ç ITaskQueueCacheManager)
        self.task_queue_manager: ITaskQueueCacheManager = TaskQueueCacheManager(redis_client=redis_client)
        logger.info("‚úÖ ArqTaskDispatcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")

    async def process_and_dispatch_tasks(
        self,
        task_list: List[Dict[str, Any]],
        batch_size: int,
        # redis_ttl_seconds: int, # –£–î–ê–õ–ï–ù–û: –≠—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
        task_arq_name: str, # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∏–º—è ARQ-–∑–∞–¥–∞—á–∏
        task_type_name: str,
    ) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –Ω–∞ –±–∞—Ç—á–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Redis –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ ARQ.
        """
        if not task_list:
            logger.info(f"–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è {task_type_name}. –ü—Ä–æ–ø—É—Å–∫ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏.")
            return []

        logger.info(f"–ù–∞—á–∞–ª–æ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏ {len(task_list)} –∑–∞–¥–∞—á –¥–ª—è {task_type_name}...")
        
        worker_batch_chunks = list(split_into_batches(task_list, batch_size))
        logger.info(f"–ó–∞–¥–∞—á–∏ {task_type_name} —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(worker_batch_chunks)} —Ä–∞–±–æ—á–∏—Ö –±–∞—Ç—á–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ {batch_size}.")

        created_batch_ids: List[str] = []
        for i, chunk_of_specs in enumerate(worker_batch_chunks):
            if not chunk_of_specs:
                continue
            
            redis_worker_batch_id = str(uuid.uuid4())
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º TaskQueueCacheManager.add_task_to_queue –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            success = await self.task_queue_manager.add_task_to_queue(
                batch_id=redis_worker_batch_id,
                key_template=KEY_PREFIX_TASK_QUEUE, # –ò—Å–ø–æ–ª—å–∑—É–µ–º KEY_PREFIX_TASK_QUEUE
                specs=chunk_of_specs,
                target_count=len(chunk_of_specs),
                initial_status="pending",
                # ttl_seconds=redis_ttl_seconds # <--- ttl_seconds —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ add_task_to_queue —Å DEFAULT_TTL_TASK_STATUS
            )
            
            if success:
                try:
                    await self.arq_redis_pool.enqueue_job(task_arq_name, redis_worker_batch_id)

                    created_batch_ids.append(redis_worker_batch_id)
                    logger.info(f"–ó–∞–¥–∞—á–∞ –¥–ª—è –±–∞—Ç—á–∞ {i+1}/{len(worker_batch_chunks)} ({task_type_name}) ID '{redis_worker_batch_id}' —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å ARQ ('{task_arq_name}').")
                except Exception as e:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –¥–ª—è –±–∞—Ç—á–∞ ID '{redis_worker_batch_id}' ({task_type_name}) –≤ –æ—á–µ—Ä–µ–¥—å ARQ: {e}", exc_info=True)
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞—Ç—á ID '{redis_worker_batch_id}' ({task_type_name}) –≤ Redis. ARQ-–∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞.")
        
        logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á –¥–ª—è {task_type_name}. –í—Å–µ–≥–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {len(created_batch_ids)} –±–∞—Ç—á–µ–π.")
        return created_batch_ids

    async def dispatch_existing_batch_id(
        self,
        batch_id: str,
        task_arq_name: str, # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∏–º—è ARQ-–∑–∞–¥–∞—á–∏
        task_type_name: str,
        *task_args: Any
    ) -> bool:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π batch_id (–∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ Redis) –≤ ARQ.
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –±–∞—Ç—á–∞ '{batch_id}' ({task_type_name}) –≤ –æ—á–µ—Ä–µ–¥—å ARQ ('{task_arq_name}').")
        try:
            await self.arq_redis_pool.enqueue_job(task_arq_name, batch_id, *task_args)
            logger.info(f"‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –±–∞—Ç—á '{batch_id}' ({task_type_name}) —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å ARQ ('{task_arq_name}').")
            return True
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –±–∞—Ç—á '{batch_id}' ({task_type_name}) –≤ –æ—á–µ—Ä–µ–¥—å ARQ: {e}", exc_info=True)
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ArqTaskDispatcher –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ FastAPI lifespan –∏–ª–∏ ARQ worker startup.
# –û–Ω –Ω–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –∑–¥–µ—Å—å –Ω–∞–ø—Ä—è–º—É—é.